# SEND NOAEL Prediction API (Traditional ML)

This project was insipred by a Hackathon event, to predict NOAEL from SEND datasets. 
This is an experiment and POC to see if we can completely develop a fully featured system to solve complex challenges using AI tools available. 
The SEND data was downloaded from phuse repo. The link is somewhere in this repo, or just google it.

Here is How I did this:
1. [Manus](https://manus.im/share/CDv3aokpoeyRxSpYVeDa7V?replay=1), helped me with creating a plan. All documents generated by Manus is also available in the docs folder. For example, see [details](docs/manus/comprehensive_report.md).
2. I then fed the entire documents to [Cursor](https://www.cursor.com/) to develop it.
3. After several iterations, and tweaks, and some handholding, here is the result: It is a working version, and [screenshots](docs/screenshots.md).
4. Also take a look at [Architecture](Architecture.md), which provides details on why TxGemma did not work.

## Overview

This project provides a FastAPI backend to predict the No Observed Adverse Effect Level (NOAEL) from preclinical toxicology studies submitted in the Standard for Exchange of Nonclinical Data (SEND) format, using a traditional Machine Learning (ML) model.

The system allows users to upload SEND datasets (as zip archives containing `.xpt` files). The backend processes these datasets, extracts relevant features, and uses a pre-trained ML model (e.g., XGBoost) to predict the NOAEL.

**Previous Approach (Deprecated):** This project initially used the TxGemma language model for prediction but pivoted to a traditional ML approach due to limitations in the model's ability to perform quantitative predictions directly from text summaries.

## Features

*   Upload SEND study data via a Zip archive.
*   Validate basic SEND domain presence.
*   Parse key domains (Demographics, Exposure, Findings - LB, TS).
*   Extract numerical features suitable for ML modeling.
*   Predict NOAEL using a pre-trained ML model (currently uses a *placeholder* XGBoost model trained on random data; predictions are not pharmacologically meaningful yet).
*   FastAPI backend with Swagger UI documentation (`/docs`).

## Project Structure

```
SEND_NOAEL_Prediction/
├── .venv/                  # Virtual environment (created by uv)
├── python/
│   ├── api/
│   │   └── main.py         # FastAPI application, endpoints
│   ├── data_processing/
│   │   ├── __init__.py
│   │   ├── send_loader.py  # Loads .xpt files from study dir
│   │   ├── domain_parser.py# Parses data from specific domains
│   │   └── feature_extractor.py # Extracts numerical features for ML
│   └── model/
│       ├── __init__.py
│       ├── saved_models/   # Directory for trained model files
│       │   └── noael_xgboost_model.joblib # Placeholder model (see Setup/Example notes)
│       └── ml_predictor.py # Loads model and performs prediction
├── uploaded_studies/       # Default location for uploaded/extracted studies
├── .gitignore
├── Architecture.md         # Detailed architecture description (Needs Update)
├── Makefile                # Convenience commands (install, run)
├── README.md               # This file
└── requirements.txt        # Python dependencies
```

## Setup

1.  **Clone the repository:**
    ```bash
    git clone <repository-url>
    cd SEND_NOAEL_Prediction
    ```

2.  **Initialize Git:**
    ```bash
    git init
    ```

3.  **Install Prerequisites:**
    *   **Python 3.10+:** Ensure you have a compatible Python version.
    *   **uv:** Install the `uv` package manager if you don't have it (`pip install uv`).
    *   **(macOS) Homebrew:** Needed to install `libomp`.
    *   **(macOS) libomp:** Install the OpenMP runtime required by XGBoost:
        ```bash
        brew install libomp
        ```

4.  **Create Virtual Environment and Install Dependencies:**
    Use the Makefile target (recommended) or run manually:
    *   **Using Makefile:**
        ```bash
        make install
        ```
    *   **Manual:**
        ```bash
        uv venv --python python3 # Or specify your python3.x version 
        source .venv/bin/activate # Or .venv\Scripts\activate on Windows
        uv pip install -r requirements.txt
        ```

5.  **Generate Placeholder Model (if needed):**
    The `requirements.txt` install includes `xgboost` and `scikit-learn`. A placeholder model file (`noael_xgboost_model.joblib`) is needed for the API pipeline to run end-to-end. **This model is trained on random data and its predictions are not meaningful.** It serves only to ensure the feature extraction and prediction pipeline works correctly. If the file doesn't exist or needs updating after feature changes, generate it:
    ```bash
    # Activate venv if not active: source .venv/bin/activate
    python3 python/model/ml_predictor.py
    ```
    *(This script creates the placeholder XGBoost model file).*

## Running the API Server (Backend)

Use the Makefile target (recommended) or run manually:

*   **Using Makefile (from project root):**
    ```bash
    make run-backend
    ```
*   **Manual (from project root):**
    ```bash
    source .venv/bin/activate # If not already active
    uvicorn python.api.main:app --reload --host 127.0.0.1 --port 8000
    ```

The API will be available at `http://127.0.0.1:8000`.
API documentation (Swagger UI) is available at `http://127.0.0.1:8000/docs`.

Keep this server running while using the frontend.

## Frontend (Next.js)

This project includes a basic Next.js frontend in the `frontend/` directory to interact with the backend API.

### Frontend Setup

1.  **Navigate to the frontend directory:**
    ```bash
    cd frontend
    ```

2.  **Install dependencies:** (Use `npm` or `yarn` based on your project setup)
    ```bash
    npm install 
    # or
    # yarn install
    ```

### Running the Frontend Development Server

1.  **Prerequisite:** Ensure the backend server is already running (see Backend section above).
2.  From the `frontend/` directory, start the Next.js development server:
    ```bash
    npm run dev
    # or
    # yarn dev
    ```
3.  Open your browser to `http://localhost:3000` (or the port indicated in the terminal).

### Frontend Configuration

The frontend needs to know the backend API URL. It currently makes requests directly to `http://127.0.0.1:8000` (as seen in `frontend/src/app/import/page.tsx`). If you change the backend port or deploy it elsewhere, you'll need to update these URLs. Using environment variables (`.env.local`) for this is recommended for more complex setups.

## Running Backend and Frontend Together

1.  Open your first terminal window, navigate to the project root (`SEND_NOAEL_Prediction/`), and start the backend:
    ```bash
    make run-backend
    ```
2.  Open a second terminal window, navigate to the frontend directory (`SEND_NOAEL_Prediction/frontend/`), and start the frontend:
    ```bash
    npm run dev 
    # or yarn dev
    ```
3.  Access the application in your browser at `http://localhost:3000`.

## Example Prediction

After uploading a study (e.g., `Vaccine-Study-1.zip`) via the `/upload/` endpoint (using the frontend's "Import Data" page or the backend's `/docs` UI), a successful call to `/predict/Vaccine-Study-1` using the current backend (with the placeholder ML model) will return a response similar to this:

```json
{
  "study_id": "Vaccine-Study-1",
  "noael_result": {
    "predicted_noael": 51.5804443359375,
    "units": "mg/kg/day",
    "model_used": "XGBoost",
    "status": "Prediction successful"
  },
  "confidence": null,
  "error": null
}
```

**Important Note:** The `predicted_noael` value shown is from the *placeholder* model (trained on random data) and is **not** pharmacologically meaningful. It only demonstrates that the prediction pipeline executed successfully. A real model must be trained on actual study data for meaningful results.

## Future Work / Improvements

*   **Train a Real ML Model:** Replace the current placeholder model by collecting a labeled dataset (SEND studies + validated NOAELs) and training the XGBoost (or other) model on this real data.
*   **Refine Feature Engineering:** Improve the feature extraction process in `feature_extractor.py` based on domain knowledge and model performance.
*   **Model Evaluation:** Implement proper model evaluation metrics.
*   **Confidence Scores:** Develop a method to estimate confidence in the ML predictions.
*   **Error Handling:** Enhance error handling and validation.
*   **Frontend:** Develop a user interface (e.g., using Next.js) for easier interaction.
*   **Configuration:** Make model paths, upload directories, etc., configurable.
*   **Testing:** Add unit and integration tests.

# SEND NOAEL Prediction API (TxGemma Demos)

To allow for comparison and exploration of Large Language Model capabilities alongside the primary traditional ML prediction, demonstration scripts based on TxGemma (originally from the `manus/` directory) are being integrated.

**Purpose:** These demos serve as a feasibility check and showcase potential text-based analyses that LLMs like TxGemma can perform on study data, contrasting with the direct quantitative prediction of the ML model.

## Integrated Demo: Automated NOAEL Determination (Simulated)

*   **Module:** `python/txgemma_demos/noael_demo.py`
*   **Functionality:** This demo analyzes key endpoints (currently Body Weight and Lab Tests) from the parsed study data using statistical tests (ANOVA, t-tests) to identify dose-dependent effects. It determines a NOAEL for each endpoint and an overall NOAEL based on the most sensitive endpoint found via this statistical analysis.
*   **LLM Simulation:** Crucially, this demo **does not** make a live call to TxGemma for the final summary. Instead, it generates a detailed text prompt summarizing the findings and then uses a template to *simulate* what an ideal LLM response might look like based on the statistical results.
*   **API Endpoint:** `POST /predict/{study_id}/txgemma_demos/noael_determination`

**Running the Demo:**

1.  Ensure the backend is running (`make run-backend`).
2.  Upload a study via the `/upload/` endpoint.
3.  Execute the `POST /predict/{study_id}/txgemma_demos/noael_determination` endpoint using the study ID (e.g., via the `/docs` UI).

**Expected Output (via API):**
The API returns a JSON response containing:
*   `overall_noael`: The NOAEL value determined by the statistical analysis (e.g., `0.0` if effects seen at lowest dose, `10.0`, etc.).
*   `dose_units`: Units for the dose (e.g., `mg/kg/day`).
*   `summary_prompt`: The detailed text prompt generated based on the analysis.
*   `simulated_response`: The template-based text simulating an LLM summary.
*   `raw_results`: The full dictionary containing intermediate steps like endpoint features, statistical results, and per-endpoint NOAELs.

**Frontend Integration (Future Work):**
*   Add a button/section in the frontend UI (e.g., on the results display page) to trigger this demo endpoint for a selected study.
*   Display the `simulated_response` text and potentially the statistically derived `overall_noael` from the demo results, clearly distinguishing it from the primary ML prediction.

**Other Demos:**
Scripts for other demos (Target Organ Prediction, Cross-Study Comparison, etc.) exist in the `manus/` directory but have not yet been integrated into the backend API or frontend UI.

